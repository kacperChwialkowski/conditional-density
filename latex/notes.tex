% LaTeX file for a 1 page document
\documentclass[10pt]{article}
\usepackage{amssymb,amsmath}
\usepackage{amsthm}

\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{Theorem}{Theorem}
\newtheorem{example}{Example}
\newtheorem{statement}{Statement}
\newtheorem{corollary}{Corollary}
\newtheorem{test}{Test}
\newtheorem{proposition}{Proposition}


\title{Conditional density estimation}


\begin{document}
\maketitle

\begin{abstract}
\end{abstract}



\section{Squared functions}
We propose the following form of the conditional density 
\begin{equation}
 p(x|y) = f(x,y)^2
\end{equation}
for $f$ in some RKHS. Further we propose a quadratic  divergence i.e.
\begin{equation}
 Q(f_0|f) = \int \int (f(x,y)^2 - f_0(x,y)^2)^2 p_0(y) dx dy. 
\end{equation}
with the constrain  
\begin{equation}
\int f(x,y)^2 dx =1 \quad \text{for all $y$ in the domain}.  
\end{equation}
Further we assume that we  are working with the product RKHS i.e. $\phi(x,y) = \phi(x) \otimes \phi(y)$. First we expand the $Q$ divergence into three terms
\begin{align}
 Q(f_0|f) &= \int f^4(x,y) p_0(y) dx dy - 2 \int f^2(x,y) f_0^2(x,y) p_0(y) dx dy  \\
 &+ \int f^4(x,y) p_0(y) dx dy 
\end{align}
Last term is some constant. The middle term can be expressed in terms of inner product with a tensor $C_2 = \int \phi(x,y) \otimes \phi(x,y) p_0(x,y) dx dy \rangle$ 
\begin{align}
\int &f^2(x,y) f_0^2(x,y) p(y) dx dy \\ 
&=\int f^2(x,y) p_0(x|y) p_0(y) dx dy    \\ 
&= \int f^2(x,y) p_0(x,y) dx dy  \\
&=\langle f \otimes f  , \int \phi(x,y) \otimes \phi(x,y) p_0(x,y) dx dy \rangle \\
&= \langle f \otimes f  , C_2 \rangle.
\end{align}
The empirical counterpart of the tensor $C_2$ is
\begin{equation}
 \overline{ C_2} = \frac 1 n \sum_{1 \leq i \leq n } \phi(Y_{i},X_{i}) \otimes \phi(Y_{i},X_{i}). 
\end{equation}
$\overline{ C_2}$ imposes some conditions about solution. Since 
\begin{equation}
 \langle f, \overline{ C_2} f \rangle = \frac 1 n \sum_{i=1}^n \langle f , \phi(X_i,Y_i) \rangle^2   
\end{equation}
so we choose solution to be spanned by the $H = \{ \phi(X_i,Y_i) \}_{1 \leq i \leq n}$. The function that lies in the span of $H$ not necessarily minimizes the divergence $Q$. Let us consider an arbitrary function $f = f_1 + f_2$, such that $f_1 \in H$ and $f_2 in H^\perp$. It is clear that  $\langle f, \overline{ C_2} f \rangle = \langle f_1, \overline{ C_2} f_1 \rangle$. On the other hand, if we consider a first term of the divergence $Q$ i.e. $D(f)(y) := \int f^4(x,y) p_0(y) dx dy$ it is not clear that 
\begin{equation}
 D(f)(y) := \int f^4(x,y) p_0(y) dx dy \leq \int f_1^4(x,y) p_0(y) dx dy.
\end{equation}
In fact such a inequality is not true. It is true  hoverer that the set of functions $\{ f: D(f)(y) =0\}$ is trivial, just because $D(f)(y) \geq 0$. We will show now that taking minimizing divergence $Q$ on the functions form the space $H$ is asymptotically consistent i.e. we recover $p_0(x|y) = f(x,y)$. We need to show that for a given $\epsilon_1$ and $\epsilon_2$ there exists $N$ such that for all $n>N$, the estimate of $f$ which we denote  $\bar f$, satisfies $P( \{ |f-\bar f| \leq \epsilon_1 \} > 1 -\epsilon_1 $ . Notice that there exists $f*$, such that $|f* - f| \leq \frac {\epsilon_1}{ 2}$ and $f* = \sum_{i=1}^D \alpha_i \phi(X_i,Y_i)$ for some finite $D$. There exists $D$ balls around points $(X_i,Y_i)$ with radius (measured in RKHS) smaller then $\frac {\epsilon_1}{ D}$. The probability of sampling a pair $X,Y$ from any of those balls is greater $p>0$, for some fixed $p$ (we assume that distribution $P_{X,Y}$ is positive on its domain). It is clear that there exists $N$ for which probability of not sampling a pair from all of the balls is arbitrarily small. Finally we notice that any function $\bar f$ spanned by the points form the balls with weights $\alpha_j$  satisfies 
$$
|\bar f - f| \leq  |\bar f - f*| + |f - f*| \leq  \frac {\epsilon_1}{ 2} + D \frac {\epsilon_1}{ D}
$$

\subsection{Empirical divergence}
Taking the solution is of the form  $f = \sum_{j=1}^{n} \alpha_j \phi(X_j,Y_j)$  we calculate the empirical divergence. The second term is
\begin{align}
  \langle f, \bar C_2 f \rangle =& \frac 1 n \sum_{i=1}^n \langle f , \phi(X_i,Y_i) \rangle^2 \\
  =& \frac 1 n \sum_{i=1}^n \left \langle \sum_{j=1}^{n} \alpha_j \phi(X_j,Y_j) , \phi(X_i,Y_i) \right \rangle^2 \\
   =& \frac 1 n \sum_{i=1}^n \left( \sum_{j=1}^{n} \alpha_j k(X_i,X_j) k(Y_j,Y_i) \right)^2
\end{align}
The inner integral of the first term, $D(f)(y)$, is 


\begin{align}
D(f)(y) &=  \int \langle f,\phi(x,y) \rangle^4 dx \\
&=\int \left( \sum_{j=1}^{n} \alpha_j k(X_j,x) k(Y_j,y) \right)^4 dx \\
&=  \sum_{j_1,j_2,j_3,j_4=1}^n  \alpha_{j_1} \alpha_{j_2} \alpha_{j_3} \alpha_{j_4}  k(Y_{j_1},y) k(Y_{j_2},y)k(Y_{j_3},y)k(Y_{j_4},y) \cdot\\
  &\quad \cdot   \left( \int k(X_{j_1},x) k(X_{j_2},x) k(X_{j_3},x) k(X_{j_4},x)  dx \right) \\
\end{align}
Then we use empirical counterpart of the outer integral 
\begin{align}
\label{eq:fst_emp}
 \int D(f)(y) p_0(y) dy \sim  \frac 1 n \sum_{i=1}^{n} D(f)(Y_i).
\end{align}
Later we will show  that this empirical integral converges to the true integral. Finally we need to calculate the constraint $\int f(x,y)^2=1$ 
\begin{align}
\label{eq:constraint}
 \int& \left( \sum_{j=1}^{n} \alpha_j  k(X_{j},x)  k(Y_{j},y) \right )^2 dx \\ 
 &= \int  \sum_{j_1,j_2=1}^{n} \alpha_{j_1} \alpha_{j_2}  k(X_{j_1},x)  k(Y_{j_1},y) k(X_{j_2},x)  k(Y_{j_2},y)  dx\\
 &=   \sum_{j_1,j_2=1}^{n} \alpha_{j_1} \alpha_{j_2}  k(Y_{j_1},y)  k(Y_{j_2},y) \int  k(X_{j_1},x)  k(X_{j_2},x)    dx  \label{eq:constraintE}
\end{align}
For the rest of the note let us assume that  
$$
\int \prod_{1<i<d} k(X_i,x) dx   = \prod_{1<i<d}  g(X_i), 
$$
which is true for e.g. Gaussian kernels. Using this we can rewrite the first term of $Q$ \eqref{eq:fst_emp} as
\begin{equation}
 \frac 1 n \sum_{i=1}^{n} D(f)(Y_i) =  \frac 1 n \sum_{i=1}^{n} \left( \sum_{j=1}^{n} \alpha_j k(Y_j,Y_i) g(X_j) \right)^4
\end{equation}
and the constraint as \eqref{eq:constraintE} as
\begin{equation}
\label{eq:const2}
\int f(x,y)^2 dx=  \left( \sum_{j=1}^{n} \alpha_j  g(X_{j})  k(Y_{j},y) \right )^2 
\end{equation}

\subsection{Empirical constraint}
It is quite unclear to me how to enforce the condition $\int f(x,y)^2dx =1$ for all $y$ (even in the form \eqref{eq:const2}). Instead I will minimize another expression (which might be not the most clever thing to do)
\begin{equation}
 I(f,f_0) = \int \left( \int f(x,y)^2 dx -1 \right)^2 p_0(y) dy
\end{equation}
After unfolding
\begin{align}
 I(f,f_0) &= \int f^4(x,y) p_0(y) dx dy - 2 \int f^2(x,y) p_0(y) dx dy +1.
\end{align}
The first term if $I$ is the same as the first term of $Q$. The second (which does nor impose any constraints on the solution) is easily derived from the equation \eqref{eq:const2} 
\begin{align}
 &\int f^2(x,y) p_0(y) dx dy =  \int \left ( \int f^2(x,y) dx \right ) p_0(y)  dy\\ 
 &\frac 1 n \sum_{i=1}^n \left( \sum_{j=1}^{n} \alpha_j  g(X_{j})  k(Y_{j},Y_i) \right )^2 .
\end{align}

To sum up we have the following problem to solve: minimize 
\begin{align}
 J&(f_0,f) +I(f,f_0) = \\
 &=\left[  \frac 1 n \sum_{i=1}^{n} \left( \sum_{j=1}^{n} \alpha_j k(Y_j,Y_i) g(X_j) \right)^4 - \frac 1 n \sum_{i=1}^n \left( \sum_{j=1}^{n} \alpha_j k(X_i,X_j) k(Y_j,Y_i) \right)^2 \right] + \\
 &+ \left[  \frac 1 n \sum_{i=1}^{n} \left( \sum_{j=1}^{n} \alpha_j k(Y_j,Y_i) g(X_j) \right)^4 - \frac 1 n \sum_{i=1}^n \left( \sum_{j=1}^{n} \alpha_j  g(X_{j})  k(Y_{j},Y_i) \right )^2 \right] \\
 &=  \frac 1 n \sum_{i=1}^{n}   \Bigg [ 2 \left( \sum_{j=1}^{n} \alpha_j k(Y_j,Y_i) g(X_j) \right)^4 - \\
  & \quad \quad -\left( \sum_{j=1}^{n} \alpha_j k(X_i,X_j) k(Y_j,Y_i) \right)^2 - \left( \sum_{j=1}^{n} \alpha_j  g(X_{j})  k(Y_{j},Y_i) \right )^2 \Bigg]   
\end{align}

\section{CCP}

So wee ccp
\begin{align}
  \frac 1 n & \sum_{i=1}^{n} \left(   \sum_{j=1}^{n} \alpha_j k(X_i,X_j) k(Y_j,Y_i) \right)^2  \\
  = & \frac 1 n  \sum_{i=1}^{n} \left(   \sum_{j=1}^{n} \alpha_j C_{i,j} \right)^2 
\end{align}

\begin{align}
  &\frac{  \partial  \frac 1 n  \sum_{i=1}^{n} \left(   \sum_{j=1}^{n} \alpha_j C_{i,j} \right)^2}{ \partial \alpha_w} \\
 & =\frac 1 n  \sum_{i=1}^{n}  \frac{ \partial \left(   \sum_{j=1}^{n} \alpha_j C_{i,j} \right)^2}{ \partial \alpha_w} \\
   &   = \frac 1 n  \sum_{i=1}^{n}   \left(   \sum_{j=1}^{n} \alpha_j C_{i,j} \right) \frac{ \partial \sum_{j=1}^{n} \alpha_j C_{i,j} }{ \partial \alpha_w} \\
   &=  \frac 1 n  \sum_{i=1}^{n}  C_{i,w} \left(   \sum_{j=1}^{n} \alpha_j C_{i,j} \right) \\
   &  \frac 1 n  \sum_{i=1}^{n}  C_{i,w} \langle C_{i\,\cdot} \alpha \rangle \\
   & = \langle C_{\cdot,w}^T , C\alpha \rangle
\end{align}
so for all $w$ we have 
$$
C^T C \alpha
$$
Silly enough if we denote  $D_{i,j} =  g(X_{j})  k(Y_{j},Y_i)$ we have other derivative 
$$
D^T D \alpha
$$
Finally the first term is $\frac 1 n \sum_{i}^{n} \langle \alpha, D_{\cdot,i} \rangle^4 = mean( (D\alpha)^{**}4)$, second is $mean( (C\alpha)^{**}2)$, 
third is $mean( (D\alpha)^{**}2)$. Each approximation will be
\begin{equation}
 mean( (C\alpha_k)^{**}4 - (C\alpha_k)^{**}2 - (D\alpha_k)^{**}2 ) - (C^T C+D^T D) \alpha_k (\alpha - \alpha_k ) 
\end{equation}


\section{Exponential families are tricky for conditional density}
If we the conditional density is written as 
\begin{equation}
p_0(x|y) = q(x,y)exp(f(x,y) - A(T(y)) )
\end{equation} 
it is hard to force that for all $y$
\begin{equation}
\label{eq:hard}
\int q(x,y)exp(f(x,y) - A(T(y)) )=1
\end{equation}
We are expecting that the solution will be of the form $f = \sum_{i=1}^n \alpha_i \frac{\partial  \phi( (X_i,Y_i), \cdot)} {\partial x} $ and it is impossible to enforce that \eqref{eq:hard} is true - see section \ref{sec:fail} for the reference.





\section{Failures}
\label{sec:fail}
Here we will assume that the conditional density can be written as 
\begin{equation}
p_0(x|y) = q(x,y)exp(f(x,y) - A(T))
\end{equation} 
By Proposition 1 from the paper [Inf] quite a few conditional densities can be approximated by functions of this type. We need to redefine Fisher divergence   
\begin{align}
J(p_0|p) &= \int \left( \int p_0(x|y) \parallel \frac{\partial  f_0(x,y)} {\partial x} -\frac{\partial  f(x,y)} {\partial x} \parallel^2 \right )p(y) dx dy  \\
&=  \int  \parallel \frac{\partial  f_0(x,y)} {\partial x} -\frac{\partial  f(x,y)} {\partial x} \parallel^2 p_0(x,y) dx dy \\
&=  \int   \left \langle f-f_0,\frac{\partial  k( (x,y), \cdot)} {\partial x} \right \rangle^2    p_0(x,y) dx dy
\end{align}
The reasoning for the Theorem 3 follows. For the sake of simplicity I assume that $x$ is one dimensional. In particular we have that 
\begin{equation}
 C := \int  p_0(x,y) \frac{\partial  k( (x,y), \cdot)} {\partial x} \otimes \frac{\partial  k( (x,y), \cdot)} {\partial x} dx dy
\end{equation}
and 
\begin{equation}
 J(f) := J(p_0|p) = \langle (f -f_0),C(f -f_0) \rangle.
\end{equation}
Also 
\begin{equation}
 J(f) = \langle f,Cf \rangle + \langle f,\xi \rangle + Const
\end{equation}
but $\xi$ is looks sightly different. Using the fact that
\begin{equation}
 \frac{\partial  f(x,y)} {\partial x}  = \frac{\partial  \log p_0(x|y)}{\partial x} - \frac{\partial  \log q(x,y)}{\partial x}
\end{equation}

\begin{align}
&\langle f,Cf_0 \rangle \\
=& \int p_0(x,y) \frac{\partial  f(x,y)} {\partial x} \frac{\partial  f_0(x,y)} {\partial x} dx dy \\
=& \int p_0(x,y) \frac{\partial  f(x,y)} {\partial x} \frac{\partial  \log p_0(x|y)}{\partial x} - \int p_0(x,y) \frac{\partial  f(x,y)} {\partial x} \frac{\partial  \log q(x,y)}{\partial x} \\
=&  \int   \left(\int  \frac{\partial  f(x,y)} {\partial x} \frac{\partial  p_0(x|y)}{\partial x} dx \right) p(y) dy 
- \int p_0(x,y) \frac{\partial  f(x,y)} {\partial x} \frac{\partial  \log q(x,y)}{\partial x} dx dy \\
\end{align}
Now the first inner  integral (check that it exists !) will be reduced by integration by parts  
\begin{align}
\int&  \frac{\partial  f(x,y)} {\partial x} \frac{\partial  p_0(x|y)}{\partial x} dx \\
&=  \frac{\partial f(x,y)} {\partial x} p_0(x|y) \mid_{-\infty}^{\infty} - \int \frac{\partial f(x,y)} {\partial x}^2 p_0(x|y)
\end{align}
we assume that $\frac{\partial f(x,y)} {\partial x} p_0(x|y) \mid_{-\infty}^{\infty} =0$, plug in and have 
\begin{align}
&\langle f,Cf_0 \rangle \\
=&  \int   \frac{\partial f(x,y)} {\partial x}^2  p_0(x,y) dx dy - \int p_0(x,y) \frac{\partial  f(x,y)} {\partial x} \frac{\partial  \log q(x,y)}{\partial x} dx dy \\
=& -\int p_0(x,y) \left \langle f , \frac{\partial  k( (x,y), \cdot)} {\partial x}^2+ \frac{\partial  k( (x,y), \cdot)} {\partial x}\frac{\partial  \log q(x,y)}{\partial x} \right \rangle
\end{align}
so 
\begin{equation}
 \xi =-\int p_0(x,y)  \left( \frac{\partial  k( (x,y), \cdot)} {\partial x}^2+ \frac{\partial  k( (x,y), \cdot)} {\partial x}\frac{\partial  \log q(x,y)}{\partial x} \right)
\end{equation}
We don't really care about the third since it is constant.


\subsection{Other option}
\label{sec:option2}
Consider a function $T(x,y)$ which is a bilinear form in some Hilbert space  $T(x,y) = \langle \phi(x), T \phi(y) \rangle $, where mapping $\phi$ and is an injection to some RKHS. Consider the following function
\begin{equation}
p(x|y) = q(x,y)exp(T(x,y) - A(T)),
\end{equation}
such that for all 
\begin{equation}
\label{eq:toOne}
 \int q(x,y)exp(T(x,y) - A(T)) = 1. 
\end{equation}
We consider Fisher divergence of $p(x|y)$ and the true conditional density $p(x|y)_0$, we suppose that $g_0$ is a true density associated with $f_0$ 
\begin{equation}
J(f_0|f) = \int g_0(x,y) \parallel \frac{\partial T(x,y)} {\partial x} -\frac{\partial T(x,y)} {\partial x} \parallel^2 dx dy  
\end{equation}







\end{document}
