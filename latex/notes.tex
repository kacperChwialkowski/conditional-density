% LaTeX file for a 1 page document
\documentclass[10pt]{article}
\usepackage{amssymb,amsmath}
\usepackage{amsthm}

\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{Theorem}{Theorem}
\newtheorem{example}{Example}
\newtheorem{statement}{Statement}
\newtheorem{corollary}{Corollary}
\newtheorem{test}{Test}
\newtheorem{proposition}{Proposition}


\title{Conditional density estimation}


\begin{document}
\maketitle

\begin{abstract}
\end{abstract}



\section{Squared functions}
We propose the following form of the conditional density 
\begin{equation}
 p(x|y) = f(x,y)^2
\end{equation}
and quadratic  divergence i.e.
\begin{equation}
 J(f_0|f) = \int \int (f(x,y)^2 - f_0(x,y)^2)^2 p_0(y) dx dy. 
\end{equation}
We will be able to enforce that 
\begin{equation}
\int f(x,y)^2 =1,  
\end{equation}
since the solution will be of a form $f = \sum_{i=1}^n \alpha_i \phi((x_i,y_i))$. We will be working with the product feature space  $\phi(x,y) = \phi(x) \otimes \phi(y)$. First we expand the divergence
\begin{align}
 J(f_0|f) &= \int f^4(x,y) p_0(y) dx dy - 2 \int f^2(x,y) f_0^2(x,y) p(y) dx dy  \\
 &+ \int f^4(x,y) p_0(y) dx dy 
\end{align}
The middle term can be expressed in terms of operator as in [Inf] 
\begin{align}
\int &f^2(x,y) f_0^2(x,y) p(y) dx dy = \int f^2(x,y) p_0(x,y) \\
 =&  \langle f \otimes f  , \int \phi(x,y) \otimes \phi(x,y) p_0(x,y) \rangle \\
 =& \langle f \otimes f  , C_2 \rangle.
\end{align}
The empirical counterpart of the tensor $C_2$ is
\begin{equation}
 \bar C_2 = \frac 1 n \sum_{1 \leq i \leq n } \phi(Y_{i},X_{i}) \otimes \phi(Y_{i},X_{i}). 
\end{equation}
$\bar C_2$ imposes some conditions about solution, since 
\begin{equation}
 \langle f, \bar C_2 f \rangle = \frac 1 n \sum_{i=1}^n \langle f , \phi(X_i,Y_i) \rangle^2   
\end{equation}
so we choose solution to be spanned by the $H = \{ \phi(X_i,Y_i) \}_{1 \leq i \leq n}$. The function that lies in the span of $H$ not necessarily minimizes the divergence $J$, but we will see that the approximate solution that belongs to $H$ converges to $f_0$. The first term does not impose any form on the solution since the following integral operator
\begin{equation}
 D_y(h) := \int h(x,y)^4 dx \geq 0
\end{equation}
has zero null space.


Suppose that divergence is minimized by some $h = h_1 + h_2$ where $h_1 \in H$ and $h_2$ sits in the orthonormal complement of $H$. Then 
\begin{equation}
 \langle h_1 + h_2 , \phi(X_i,Y_i) \rangle^2 =\langle h_1 , \phi(X_i,Y_i) \rangle
\end{equation}
so the second term is not getting smaller. Adding $h_2$ to the solution might minimize the first term, but it is unclear what should be added. We will show that as we increase the number of the observations we are getting closer to a function $h_1 + h_2$ that minimizes the whole expression.
For now we calculate the empirical divergence. The second part is
\begin{align}
  \langle f, \bar C_2 f \rangle = \frac 1 n \sum_{i=1}^n \langle f , \phi(X_i,Y_i) \rangle^2 \\
  = \frac 1 n \sum_{i=1}^n \langle \sum_{j=1}^{n} \alpha_j \phi(X_j,Y_j) , \phi(X_i,Y_i) \rangle^2 \\
   = \frac 1 n \sum_{i,j_1,j_2=1}^n \alpha_{j_1} \alpha_{j_2} k(X_{j_1},X_i) k(X_{j_2},X_i)  k(Y_{j_1},Y_i) k(Y_{j_2},Y_i) 
\end{align}
And the first term looks like that 
\begin{align}
  \frac 1 n \sum_{i=1}^n \langle  (\otimes f)^4  ,  \left( \int  (\otimes \phi(x))^4 dx  \right)  \otimes \left(    (\otimes \phi(Y_{i}))^4 \right) \rangle \\
  = \frac 1 n \sum_{i=1}^n \langle  ( \sum_{j=1}^{n} \alpha_j \phi(X_j,Y_j))^4  ,  \left( \int  (\otimes \phi(x))^4 dx  \right)  \otimes \left(    (\otimes \phi(Y_{i}))^4 \right) \rangle \\
  =\frac 1 n \sum_{i,j_1,j_2,j_3,j_4=1}^n  \alpha_{j_1} \alpha_{j_2} \alpha_{j_3} \alpha_{j_4} \langle \phi(X_{j_1}) \otimes (Y_{j_1}) \otimes  \phi(X_{j_2}) \otimes (Y_{j_2}) \otimes  \phi(X_{j_3}) \otimes (Y_{j_3}) \otimes  \phi(X_{j_4}) \otimes (Y_{j_4})\\
  ,\left( \int  (\otimes \phi(x))^4 dx  \right)  \otimes \left(    \otimes \phi(Y_{i}) \right)^4 \rangle \\
  =\frac 1 n \sum_{i,j_1,j_2,j_3,j_4=1}^n  \alpha_{j_1} \alpha_{j_2} \alpha_{j_3} \alpha_{j_4} \left( \int k(X_{j_1},x) k(X_{j_2},x) k(X_{j_3},x) k(X_{j_4},x)  dx \right) \\ 
    k(Y_{j_1},Y_i) k(Y_{j_2},Y_i)k(Y_{j_3},Y_i)k(Y_{j_4},Y_i)
\end{align}
This integral is a sad little thing, but basically there are two cases. Either after integration we can factorize the result or not. If we can (we can for Gaussian kernel ) we have  
\begin{align}
 =\frac 1 n \sum_{i,j_1,j_2,j_3,j_4=1}^n  \alpha_{j_1} \alpha_{j_2} \alpha_{j_3} \alpha_{j_4}  f(X_{j_1}) f(X_{j_2}) f(X_{j_3}) f(X_{j_4})   \\ 
    k(Y_{j_1},Y_i) k(Y_{j_2},Y_i)k(Y_{j_3},Y_i)k(Y_{j_4},Y_i)
\end{align}
Finally we need to calculate the constraint
\begin{align}
 \int \left( \sum_{j=1}^{n} \alpha_j  k(X_{j},x)  k(Y_{j},y) \right )^2 dx \\ 
 = \int  \sum_{j,i=1}^{n} \alpha_j  k(X_{j},x)  k(Y_{j},y) k(X_{i},x)  k(Y_{i},y)  dx\\
 =   \sum_{j,i=1}^{n} \alpha_j  f(X_{j},x)  k(Y_{j},y) f(X_{i},x)  k(Y_{i},y)  \\
\end{align}
The latter must be equal to to.

Now we need to prove that we recover optimal solution asymptotically. We know that $f$ can be written as $f = \sum_{i=1}^{D} \phi(X_i,Y_i)$. Set $D$ trap-balls of radius $\epsilon/D$ around those $D$ points and wait until all traps are non-empty. Using Borel-Cantelli we know that we will succeed since the probability of falling into any of the traps is positive.          



\section{Exponential families are tricky for conditional density}
If we the conditional density is written as 
\begin{equation}
p_0(x|y) = q(x,y)exp(f(x,y) - A(T(y)) )
\end{equation} 
it is hard to force that for all $y$
\begin{equation}
\label{eq:hard}
\int q(x,y)exp(f(x,y) - A(T(y)) )=1
\end{equation}
We are expecting that the solution will be of the form $f = \sum_{i=1}^n \alpha_i \frac{\partial  \phi( (X_i,Y_i), \cdot)} {\partial x} $ and it is impossible to enforce that \eqref{eq:hard} is true - see section \ref{sec:fail} for the reference.





\section{Failures}
\label{sec:fail}
Here we will assume that the conditional density can be written as 
\begin{equation}
p_0(x|y) = q(x,y)exp(f(x,y) - A(T))
\end{equation} 
By Proposition 1 from the paper [Inf] quite a few conditional densities can be approximated by functions of this type. We need to redefine Fisher divergence   
\begin{align}
J(p_0|p) &= \int \left( \int p_0(x|y) \parallel \frac{\partial  f_0(x,y)} {\partial x} -\frac{\partial  f(x,y)} {\partial x} \parallel^2 \right )p(y) dx dy  \\
&=  \int  \parallel \frac{\partial  f_0(x,y)} {\partial x} -\frac{\partial  f(x,y)} {\partial x} \parallel^2 p_0(x,y) dx dy \\
&=  \int   \left \langle f-f_0,\frac{\partial  k( (x,y), \cdot)} {\partial x} \right \rangle^2    p_0(x,y) dx dy
\end{align}
The reasoning for the Theorem 3 follows. For the sake of simplicity I assume that $x$ is one dimensional. In particular we have that 
\begin{equation}
 C := \int  p_0(x,y) \frac{\partial  k( (x,y), \cdot)} {\partial x} \otimes \frac{\partial  k( (x,y), \cdot)} {\partial x} dx dy
\end{equation}
and 
\begin{equation}
 J(f) := J(p_0|p) = \langle (f -f_0),C(f -f_0) \rangle.
\end{equation}
Also 
\begin{equation}
 J(f) = \langle f,Cf \rangle + \langle f,\xi \rangle + Const
\end{equation}
but $\xi$ is looks sightly different. Using the fact that
\begin{equation}
 \frac{\partial  f(x,y)} {\partial x}  = \frac{\partial  \log p_0(x|y)}{\partial x} - \frac{\partial  \log q(x,y)}{\partial x}
\end{equation}

\begin{align}
&\langle f,Cf_0 \rangle \\
=& \int p_0(x,y) \frac{\partial  f(x,y)} {\partial x} \frac{\partial  f_0(x,y)} {\partial x} dx dy \\
=& \int p_0(x,y) \frac{\partial  f(x,y)} {\partial x} \frac{\partial  \log p_0(x|y)}{\partial x} - \int p_0(x,y) \frac{\partial  f(x,y)} {\partial x} \frac{\partial  \log q(x,y)}{\partial x} \\
=&  \int   \left(\int  \frac{\partial  f(x,y)} {\partial x} \frac{\partial  p_0(x|y)}{\partial x} dx \right) p(y) dy 
- \int p_0(x,y) \frac{\partial  f(x,y)} {\partial x} \frac{\partial  \log q(x,y)}{\partial x} dx dy \\
\end{align}
Now the first inner  integral (check that it exists !) will be reduced by integration by parts  
\begin{align}
\int&  \frac{\partial  f(x,y)} {\partial x} \frac{\partial  p_0(x|y)}{\partial x} dx \\
&=  \frac{\partial f(x,y)} {\partial x} p_0(x|y) \mid_{-\infty}^{\infty} - \int \frac{\partial f(x,y)} {\partial x}^2 p_0(x|y)
\end{align}
we assume that $\frac{\partial f(x,y)} {\partial x} p_0(x|y) \mid_{-\infty}^{\infty} =0$, plug in and have 
\begin{align}
&\langle f,Cf_0 \rangle \\
=&  \int   \frac{\partial f(x,y)} {\partial x}^2  p_0(x,y) dx dy - \int p_0(x,y) \frac{\partial  f(x,y)} {\partial x} \frac{\partial  \log q(x,y)}{\partial x} dx dy \\
=& -\int p_0(x,y) \left \langle f , \frac{\partial  k( (x,y), \cdot)} {\partial x}^2+ \frac{\partial  k( (x,y), \cdot)} {\partial x}\frac{\partial  \log q(x,y)}{\partial x} \right \rangle
\end{align}
so 
\begin{equation}
 \xi =-\int p_0(x,y)  \left( \frac{\partial  k( (x,y), \cdot)} {\partial x}^2+ \frac{\partial  k( (x,y), \cdot)} {\partial x}\frac{\partial  \log q(x,y)}{\partial x} \right)
\end{equation}
We don't really care about the third since it is constant.


\subsection{Other option}
\label{sec:option2}
Consider a function $T(x,y)$ which is a bilinear form in some Hilbert space  $T(x,y) = \langle \phi(x), T \phi(y) \rangle $, where mapping $\phi$ and is an injection to some RKHS. Consider the following function
\begin{equation}
p(x|y) = q(x,y)exp(T(x,y) - A(T)),
\end{equation}
such that for all 
\begin{equation}
\label{eq:toOne}
 \int q(x,y)exp(T(x,y) - A(T)) = 1. 
\end{equation}
We consider Fisher divergence of $p(x|y)$ and the true conditional density $p(x|y)_0$, we suppose that $g_0$ is a true density associated with $f_0$ 
\begin{equation}
J(f_0|f) = \int g_0(x,y) \parallel \frac{\partial T(x,y)} {\partial x} -\frac{\partial T(x,y)} {\partial x} \parallel^2 dx dy  
\end{equation}







\end{document}
